#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""created 30.04.2021
@author: holger mohr
"""

import numpy as np
import torch
from torch import nn


class SimpleEnv:
    
    def __init__(self, rng):
        
        self.rng = rng
        self.screen = np.array([1, 0])
        self.frame_no = 0
    
    def __call__(self, action):
        
        if all(self.screen == np.array([1, 0])):
            reward_vec = np.array([1, 0])
        elif all(self.screen == np.array([0, 1])):
            reward_vec = np.array([0, 2])
        
        reward = reward_vec[action]
        
        draw_screen = self.rng.integers(2)
        
        if draw_screen == 0:
            screen = np.array([1, 0])
        elif draw_screen == 1:
            screen = np.array([0, 1])
        
        self.screen = screen
        self.frame_no += 1
        
        return reward


class QAgentManual:
    
    def __init__(self, rng, eps_rand, learning_rate):
        
        self.rng = rng
        self.eps_rand = eps_rand
        self.learning_rate = learning_rate
        self.Q = torch.tensor([[0.5, 0.5], [0.5, 0.5]])
        self.reward_pred = self.Q[0,0]
        self.screen = np.array([1, 0])
        self.action = 0
    
    def __call__(self, screen):  # forward pass
        
        self.screen = screen
        screen_tensor = torch.tensor(screen, dtype=torch.float32)
        q_vec = torch.matmul(self.Q, screen_tensor)
        action = self.get_response_from_q(q_vec)
        self.action = action
        self.reward_pred = q_vec[action]
        return action
    
    def get_response_from_q(self, q_vec):
        
        if self.rng.random() < self.eps_rand:
            
            action = self.rng.integers(2)
            
        else:
            
            if q_vec[0] == q_vec[1]:
                action = self.rng.integers(2)
            else:
                action = torch.argmax(q_vec).numpy()
        
        self.eps_rand = max(self.eps_rand - 0.01, 0)
        
        return action
    
    def qlearning_manual(self, reward):
        
        reward_torch = torch.tensor(reward, dtype=torch.float32)
        
        if all(self.screen == np.array([1, 0])) and self.action == 0:
            self.Q[0,0] -= self.learning_rate*(self.reward_pred - reward_torch)
        elif all(self.screen == np.array([1, 0])) and self.action == 1:
            self.Q[1,0] -= self.learning_rate*(self.reward_pred - reward_torch)
        elif all(self.screen == np.array([0, 1])) and self.action == 0:
            self.Q[0,1] -= self.learning_rate*(self.reward_pred - reward_torch)
        elif all(self.screen == np.array([0, 1])) and self.action == 1:
            self.Q[1,1] -= self.learning_rate*(self.reward_pred - reward_torch)


class QAgentAutograd:
    
    def __init__(self, rng, eps_rand, learning_rate):
        
        self.rng = rng
        self.eps_rand = eps_rand
        self.learning_rate = learning_rate
        self.Q = torch.tensor([[0.5, 0.5], [0.5, 0.5]], requires_grad=True)
        self.reward_pred = self.Q[0,0]
    
    def __call__(self, screen):  # forward pass
        
        screen_tensor = torch.tensor(screen, dtype=torch.float32)
        q_vec = torch.matmul(self.Q, screen_tensor)
        action = self.get_response_from_q(q_vec)
        self.reward_pred = q_vec[action]
        return action
    
    def get_response_from_q(self, q_vec):
        
        if self.rng.random() < self.eps_rand:
            
            action = self.rng.integers(2)
            
        else:
            
            if q_vec[0] == q_vec[1]:
                action = self.rng.integers(2)
            else:
                action = torch.argmax(q_vec).numpy()
        
        self.eps_rand = max(self.eps_rand - 0.01, 0)
        
        return action
    
    def qlearning_autograd(self, reward):
        
        reward_torch = torch.tensor(reward, dtype=torch.float32)
        
        loss = 0.5*(self.reward_pred - reward_torch).pow(2)
        self.Q.grad = None
        loss.backward()
        with torch.no_grad():
            self.Q -= self.learning_rate*self.Q.grad


class QAgentNN:
    
    def __init__(self, rng, eps_rand, learning_rate):
        
        self.rng = rng
        self.eps_rand = eps_rand
        self.learning_rate = learning_rate
        self.QNN = torch.nn.Sequential(torch.nn.Linear(2, 2, bias=False))
        nn.init.constant_(self.QNN[0].weight, 0.5)
        self.reward_pred = self.QNN[0].weight[0,0]
        self.loss_fn = torch.nn.MSELoss()
    
    def __call__(self, screen):  # forward pass
        
        screen_tensor = torch.tensor(screen, dtype=torch.float32)
        q_vec = self.QNN(screen_tensor)
        action = self.get_response_from_q(q_vec)
        self.reward_pred = q_vec[action]
        return action
    
    def get_response_from_q(self, q_vec):
        
        if self.rng.random() < self.eps_rand:
            
            action = self.rng.integers(2)
            
        else:
            
            if q_vec[0] == q_vec[1]:
                action = self.rng.integers(2)
            else:
                action = torch.argmax(q_vec).numpy()
        
        self.eps_rand = max(self.eps_rand - 0.01, 0)
        
        return action
    
    def qlearning_nn(self, reward):
        
        reward_torch = torch.tensor(reward, dtype=torch.float32)
        
        loss = 0.5*self.loss_fn(self.reward_pred, reward_torch)
        self.QNN.zero_grad()
        loss.backward()
        with torch.no_grad():
            self.QNN[0].weight -= self.learning_rate*self.QNN[0].weight.grad


class QAgentOptim:
    
    def __init__(self, rng, eps_rand, learning_rate):
        
        self.rng = rng
        self.eps_rand = eps_rand
        self.learning_rate = learning_rate
        self.QNN = torch.nn.Sequential(torch.nn.Linear(2, 2, bias=False))
        nn.init.constant_(self.QNN[0].weight, 0.5)
        self.reward_pred = self.QNN[0].weight[0,0]
        self.loss_fn = torch.nn.MSELoss()
        self.optimizer = torch.optim.SGD(self.QNN.parameters(), lr=self.learning_rate)
    
    def __call__(self, screen):  # forward pass
        
        screen_tensor = torch.tensor(screen, dtype=torch.float32)
        q_vec = self.QNN(screen_tensor)
        action = self.get_response_from_q(q_vec)
        self.reward_pred = q_vec[action]
        return action
    
    def get_response_from_q(self, q_vec):
        
        if self.rng.random() < self.eps_rand:
            
            action = self.rng.integers(2)
            
        else:
            
            if q_vec[0] == q_vec[1]:
                action = self.rng.integers(2)
            else:
                action = torch.argmax(q_vec).numpy()
        
        self.eps_rand = max(self.eps_rand - 0.01, 0)
        
        return action
    
    def qlearning_optim(self, reward):
        
        reward_torch = torch.tensor(reward, dtype=torch.float32)
        
        loss = 0.5*self.loss_fn(self.reward_pred, reward_torch)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


def main():
    
    rng_manual = np.random.default_rng(1)
    rng_autograd = np.random.default_rng(1)
    rng_nn = np.random.default_rng(1)
    rng_optim = np.random.default_rng(1)
    
    env_manual = SimpleEnv(rng_manual)
    env_autograd = SimpleEnv(rng_autograd)
    env_nn = SimpleEnv(rng_nn)
    env_optim = SimpleEnv(rng_optim)
    
    frame_no_manual = env_manual.frame_no
    frame_no_autograd = env_autograd.frame_no
    frame_no_nn = env_nn.frame_no
    frame_no_optim = env_optim.frame_no
    
    n_frames = 100
    eps_rand = 1.0
    learning_rate = 0.1
    
    action_manual = 0
    action_autograd = 0
    action_nn = 0
    action_optim = 0
    
    q_agent_manual = QAgentManual(rng_manual, eps_rand, learning_rate)
    q_agent_autograd = QAgentAutograd(rng_autograd, eps_rand, learning_rate)
    q_agent_nn = QAgentNN(rng_nn, eps_rand, learning_rate)
    q_agent_optim = QAgentOptim(rng_optim, eps_rand, learning_rate)
    
    while frame_no_manual < n_frames:
        
        frame_no_manual = env_manual.frame_no
        screen_old_manual = env_manual.screen
        reward_manual = env_manual(action_manual)
        
        print(' ')
        print('************************************')
        
        print(' ')
        print('QAgentManual:')
        print('frame no: ' + str(frame_no_manual))
        print('eps_rand: ' + str(q_agent_manual.eps_rand))
        print('screen: ' + str(screen_old_manual))
        print('action: ' + str(action_manual))
        print('reward: ' + str(reward_manual))
        
        frame_no_autograd = env_autograd.frame_no
        screen_old_autograd = env_autograd.screen
        reward_autograd = env_autograd(action_autograd)
        
        print(' ')
        print('QAgentAutograd:')
        print('frame no: ' + str(frame_no_autograd))
        print('eps_rand: ' + str(q_agent_autograd.eps_rand))
        print('screen: ' + str(screen_old_autograd))
        print('action: ' + str(action_autograd))
        print('reward: ' + str(reward_autograd))
        
        frame_no_nn = env_nn.frame_no
        screen_old_nn = env_nn.screen
        reward_nn = env_nn(action_nn)
        
        print(' ')
        print('QAgentNN:')
        print('frame no: ' + str(frame_no_nn))
        print('eps_rand: ' + str(q_agent_nn.eps_rand))
        print('screen: ' + str(screen_old_nn))
        print('action: ' + str(action_nn))
        print('reward: ' + str(reward_nn))
        
        frame_no_optim = env_optim.frame_no
        screen_old_optim = env_optim.screen
        reward_optim = env_optim(action_optim)
        
        print(' ')
        print('QAgentOptim:')
        print('frame no: ' + str(frame_no_optim))
        print('eps_rand: ' + str(q_agent_optim.eps_rand))
        print('screen: ' + str(screen_old_optim))
        print('action: ' + str(action_optim))
        print('reward: ' + str(reward_optim))
        
        
        """ learning """
        
        print(' ')
        print('Q:')
        print(q_agent_manual.Q)
        print(' ')
        print('Q_with_grad:')
        print(q_agent_autograd.Q)
        print('Q_with_grad.grad:')
        print(q_agent_autograd.Q.grad)
        print(' ')
        print('QNN:')
        print(q_agent_nn.QNN[0].weight)
        print('QNN grad:')
        print(q_agent_nn.QNN[0].weight.grad)
        print(' ')
        print('QNN optim:')
        print(q_agent_optim.QNN[0].weight)
        print('QNN optim grad:')
        print(q_agent_optim.QNN[0].weight.grad)
        print(' ')
        
        q_agent_manual.qlearning_manual(reward_manual)
        q_agent_autograd.qlearning_autograd(reward_autograd)
        q_agent_nn.qlearning_nn(reward_nn)
        q_agent_optim.qlearning_optim(reward_optim)
        
        """ forward pass """
        
        screen_new_manual = env_manual.screen
        screen_new_autograd = env_autograd.screen
        screen_new_nn = env_nn.screen
        screen_new_optim = env_optim.screen
        
        action_manual = q_agent_manual(screen_new_manual)
        action_autograd = q_agent_autograd(screen_new_autograd)
        action_nn = q_agent_nn(screen_new_nn)
        action_optim = q_agent_optim(screen_new_optim)
        
        frame_no_manual = env_manual.frame_no


if __name__ == '__main__':
    main()
