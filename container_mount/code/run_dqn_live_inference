#!/usr/bin/env python3

# -*- coding: utf-8 -*-

"""created 16.08.2021
@author: holger mohr
"""


from ale_python_interface import ALEInterface
import numpy as np
from PIL import Image
import torch
from torch import nn
import copy
import time
import fcntl


class DqnNN(nn.Module):
    
    def __init__(self, no_of_actions):
        super(DqnNN, self).__init__()
        
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(3136, 512),
            nn.ReLU(),
            nn.Linear(512, no_of_actions))
    
    def forward(self, x):
        conv_out = self.conv(x)
        return conv_out


def ale_init(rng):
    
    max_frames_per_episode = 50000
    
    ale = ALEInterface()
    ale.loadROM(str.encode('/workspace/container_mount/roms/space_invaders.bin'))
    ale.setInt(b'max_num_frames_per_episode', max_frames_per_episode)
    minimal_actions = ale.getMinimalActionSet()
    (screen_width,screen_height) = ale.getScreenDims()
    ale_seed = rng.integers(2^32)
    ale.setInt(b'random_seed', ale_seed)
    random_seed = ale.getInt(b'random_seed')
    ale.setFloat(b'repeat_action_probability', 0.0)
    action_repeat_prob = ale.getFloat(b'repeat_action_probability')
    
    print('width/height: ' +str(screen_width) + '/' + str(screen_height))
    print('max frames per episode: ' + str(max_frames_per_episode))
    print('minimal actions: ' + str(minimal_actions))
    print('random seed: ' + str(random_seed))
    print('action repeat prob.: ' + str(action_repeat_prob))
    
    return ale


def ale_15hz_live(action, last_15hz_screen):
    
    #(screen_width,screen_height) = ale.getScreenDims()
    
    #screen_vec_1 = np.empty((screen_height, screen_width, 3), dtype=np.uint8)
    #screen_vec_2 = np.empty((screen_height, screen_width, 3), dtype=np.uint8)
    
    reward_sum = 0
    
    #reward = ale.act(action)
    #reward_sum += reward
    
    #reward = ale.act(action)
    #reward_sum += reward
    
    #reward = ale.act(action)
    #reward_sum += reward
    #ale.getScreenRGB(screen_vec_1)
    
    #reward = ale.act(action)
    #reward_sum += reward
    #ale.getScreenRGB(screen_vec_2)
    
    screen_vec_1 = last_15hz_screen[:,:,:,0]
    screen_vec_2 = last_15hz_screen[:,:,:,1]
    
    screen_R_max = np.amax(np.dstack((screen_vec_1[:,:,0], screen_vec_2[:,:,0])), axis=2)
    screen_G_max = np.amax(np.dstack((screen_vec_1[:,:,1], screen_vec_2[:,:,1])), axis=2)
    screen_B_max = np.amax(np.dstack((screen_vec_1[:,:,2], screen_vec_2[:,:,2])), axis=2)
    
    screen_max = np.dstack((screen_R_max, screen_G_max, screen_B_max))
    
    return screen_max, reward_sum


def preproc_screen(screen_np_in):
    
    screen_image = Image.fromarray(screen_np_in, 'RGB')
    screen_ycbcr = screen_image.convert('YCbCr')
    screen_y = screen_ycbcr.getchannel(0)
    screen_y_84x84 = screen_y.resize((84, 84), resample=Image.BILINEAR)
    screen_y_84x84_float_rescaled_np = np.array(screen_y_84x84, dtype=np.float32)/255.0
    
    return screen_y_84x84_float_rescaled_np


class DqnAgent():
    
    def __init__(self, rng, minimal_actions):
        
        self.rng = rng
        self.minimal_actions = minimal_actions
        self.eps_param = 1.0
        self.eps_param_eval = 0.05
        self.experience_replay_buffer_size = 200000
        self.experience_replay_buffer_circle_ind = 0
        self.experience_replay_buffer = []
        self.state = np.zeros((4, 84, 84), dtype=np.float32)
        self.action_ind = 0
        self.action_count = 0
        self.network = DqnNN(len(minimal_actions)).to('cuda')
        #self.network_target = DqnNN(len(minimal_actions)).to('cuda')
        self.network_target_update_freq = 10000
        self.train_network_stepsize = 4
        self.batch_size = 32
        self.discount_gamma = torch.tensor(0.99)
        self.lr_value = 0.00025
        self.loss_fn = torch.nn.SmoothL1Loss()
        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.lr_value)
        self.clip_rewards_flag = True
        
        #TODO: CHECK WHETHER DEEPCOPY IS REQUIRED INSTEAD?!?
        #self.network_target.load_state_dict(self.network.state_dict())
        self.network_target = copy.deepcopy(self.network)
    
    
    def __call__(self, screen, reward, train_flag):
        
        state_new = self.update_state(screen)
        
        if train_flag:
            self.update_experience_replay_buffer(state_new, reward)
        
        action, self.action_ind = self.act_eps_greedy(state_new, train_flag)
        self.state = state_new
        
        if train_flag and self.action_count >= self.experience_replay_buffer_size and self.action_count % self.train_network_stepsize == 0:
            self.train_network()
        
        return action
    
    def update_state(self, screen):
    
        state_new = np.concatenate((np.expand_dims(screen, axis=0), self.state[0:3, :,:]))
        
        return state_new
    
    def update_experience_replay_buffer(self, state_new, reward):
        
        if len(self.experience_replay_buffer) < self.experience_replay_buffer_size:
            self.experience_replay_buffer.append((self.state, state_new, self.action_ind, reward))
        else:
            self.experience_replay_buffer[self.experience_replay_buffer_circle_ind] = (self.state, state_new, self.action_ind, reward)
        self.experience_replay_buffer_circle_ind = (self.experience_replay_buffer_circle_ind + 1) % self.experience_replay_buffer_size
    
    def act_eps_greedy(self, state_new, train_flag):
        
        if train_flag:
            eps_param_value = self.eps_param
        else:
            eps_param_value = self.eps_param_eval
        
        if self.rng.random() < eps_param_value:
            action_index = self.rng.integers(len(self.minimal_actions))  # 0, 1, ..., no_of_act-1
        else:
            state_tensor = torch.from_numpy(state_new).cuda().unsqueeze(0)
            with torch.no_grad():
                net_out = self.network(state_tensor)
            action_index = torch.argmax(net_out).cpu().numpy()
        
        action = self.minimal_actions[action_index]
        
        if train_flag:
            self.eps_param = max(1.0*(1.0 - self.action_count/1e6) + 0.1*self.action_count/1e6, 0.1)
            self.action_count += 1
        
        return action, action_index
    
    def sample_batch(self):
        
        state_batch_np = np.empty((self.batch_size, 4, 84, 84), dtype=np.float32)
        state_next_batch_np = np.empty((self.batch_size, 4, 84, 84), dtype=np.float32)
        action_ind_batch_np = np.empty(self.batch_size, dtype=np.int64)
        reward_batch_np = np.empty(self.batch_size, dtype=np.float32)
        
        for sample_no in range(self.batch_size):
            
            random_sample_ind = self.rng.integers(self.experience_replay_buffer_size)
            (state, state_next, action_ind, reward) = self.experience_replay_buffer[random_sample_ind]
            state_batch_np[sample_no,:,:,:] = state
            state_next_batch_np[sample_no,:,:,:] = state_next
            action_ind_batch_np[sample_no] = action_ind
            reward_batch_np[sample_no] = reward
        
        return state_batch_np, state_next_batch_np, action_ind_batch_np, reward_batch_np
    
    def clip_reward(self, reward_vec):
        
        for i in range(len(reward_vec)):
            if reward_vec[i] > 0.0:
                reward_vec[i] = 1.0
            elif reward_vec[i] < 0.0:
                reward_vec[i] = -1.0
        
        return reward_vec
    
    def train_network(self):
        
        if int(self.action_count) % self.network_target_update_freq == 0:
            #TODO: CHECK WHETHER DEEPCOPY IS REQUIRED INSTEAD?!?
            #self.network_target.load_state_dict(self.network.state_dict())
            self.network_target = copy.deepcopy(self.network)
        
        state_batch_np, state_next_batch_np, action_ind_batch_np, reward_batch_np = self.sample_batch()
        
        if self.clip_rewards_flag:
            reward_batch_np = self.clip_reward(reward_batch_np)
        
        state_batch_tensor = torch.from_numpy(state_batch_np).cuda()
        state_next_batch_tensor = torch.from_numpy(state_next_batch_np).cuda()
        
        with torch.no_grad():
            net_out_state_next = self.network_target(state_next_batch_tensor)
            action_ind_max = torch.argmax(net_out_state_next, axis=1)
            q_next_max = net_out_state_next[np.arange(self.batch_size), action_ind_max]
            q_pred = torch.from_numpy(reward_batch_np).cuda() + self.discount_gamma * q_next_max
        
        net_out_state = self.network(state_batch_tensor)
        q_actual = net_out_state[np.arange(self.batch_size), action_ind_batch_np]
        
        loss = self.loss_fn(q_actual, q_pred)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


def main():
    
    rng = np.random.default_rng()
    
    ale = ale_init(rng)
    
    minimal_actions = ale.getMinimalActionSet()
    
    dqn_agent = DqnAgent(rng, minimal_actions)
    
    #torch.save(dqn_agent.network.state_dict(), '/workspace/container_mount/data/space_inv_DNN_epoch_' + str(epoch_no + 1).zfill(3) + '.pt')
    dqn_agent.network.load_state_dict(torch.load('/workspace/container_mount/data/space_inv_DNN_epoch_130.pt'))
    dqn_agent.network.eval()
    
    n_frames_15hz = 6300
    
    action = 0
    
    response_flag_1 = np.ones((1, 1), dtype=np.uint8)
    screen_flag_0 = np.zeros((1, 1), dtype=np.uint8)
    action_ind_ALE = np.zeros((1, 1), dtype=np.uint8)
    
    time_start = time.time()
    
    for frame_no in range(n_frames_15hz):
        
        repeat_wait_flag = True
        
        while repeat_wait_flag:
            fd_screen = open('/workspace/container_mount/ramdisk/screenlock', 'r')
            fcntl.flock(fd_screen, fcntl.LOCK_EX)
            screen_flag_csv = np.loadtxt(open("/workspace/container_mount/ramdisk/screen_flag.csv"))
            fcntl.flock(fd_screen, fcntl.LOCK_UN)
            fd_screen.close()
            
            if (screen_flag_csv == 1):
                time_end = time.time()
                print(frame_no)
                print(time_end - time_start)
                print(" ")
                time_start = time.time()
                np.savetxt("/workspace/container_mount/ramdisk/screen_flag.csv", screen_flag_0, fmt='%01.0u')
                repeat_wait_flag = False
            else:
                time.sleep(0.01)
                
        last_15hz_screen = np.load('/workspace/container_mount/ramdisk/last_15hz_screen.npy')
        
        screen_RGB_15hz, reward_15hz = ale_15hz_live(action, last_15hz_screen)
        
        screen_preproc_15hz = preproc_screen(screen_RGB_15hz)
        
        action = dqn_agent(screen_preproc_15hz, reward_15hz, False)
        
        print(action)
        
        action_ind_ALE[0] = action

        np.savetxt('/workspace/container_mount/ramdisk/action_ind_ALE.csv', action_ind_ALE, fmt='%01.0u')
        
        fd_response = open('/workspace/container_mount/ramdisk/responselock', 'r')
        fcntl.flock(fd_response, fcntl.LOCK_EX)
        np.savetxt('/workspace/container_mount/ramdisk/response_flag.csv', response_flag_1, fmt='%01.0u')
        fcntl.flock(fd_response, fcntl.LOCK_UN)
        fd_response.close()


if __name__ == '__main__':
    main()
    
